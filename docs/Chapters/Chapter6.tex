

\chapter{Conclusiones} % Main chapter title

\label{Conclusiones} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
En este trabajo se ha presentado un estado del arte en temas de seguridad en redes neuronales artificiales, que como se muestra en el documento, es un tema que ha tomado gran interés científico en los últimos años. Se han estudiado, citado y relacionado diversas investigaciones que muestran las vulnerabilidades que presentan los sistemas de inteligencia artificial, con particular énfasis en la sensibilidad de los modelos de clasificación al ser atacadas con pequeñas perturbaciones en sus entradas, las que incluso pueden ser imperceptibles a la revisión humana. También se presentaron técnicas que intentan dar robustez a los modelos frente a dichos ataques, pero estas solo brindan una protección parcial para ciertos tipos de ataques particulares. La seguridad en las redes neuronales es un tema abierto al día de hoy y diversos equipos de investigación se encuentran trabajando en la búsqueda de una solución que garantice dicha seguridad, pero también se siguen descubriendo nuevas técnicas de ataques.

En los experimentos realizados se pudo observar que ciertos modelos fueron fácilmente vulnerados. Se logró perpetuar un ataque de caja blanca en un modelo propio, como también en un modelo importado pre-entrenado con una compleja arquitectura y ampliamente usado, haciendo uso la función de gradiente descendiente de optimización de la red de manera invertida. Luego se demostró que las entradas generadas para el ataque pueden ser utilizadas para efectuar nuevos ataques a otros modelos que implementan el mismo tipo de clasificación, pero distintas arquitecturas, debido a una estudiada característica de las redes neuronales profundas llamadas transferibilidad en ataques de caja negra. Luego utilizando los mismos conjuntos de datos, se realizó una comparativa de los resultados de un ataque utilizando una de las técnicas presentadas en el documento y un ataque con manipulación aleatoria, demostrando la efectividad del primero.

También se realizaron experimentos usando imágenes de señaléticas de tránsito para resaltar los peligros que conlleva el uso de esta tecnología en las aplicaciones de conducción automática de vehículos, aportando además estudios que detallan sus riesgos, como también noticias recientes de incidentes relacionados a esta tecnología. Las imágenes manipuladas para el ataque presentan perturbaciones imperceptibles a la vista humana, pero de gran impacto en el resultado del clasificador de la red neuronal profunda.

En otro experimento realizado en este estudio, se usó como víctima de ataque un sistema que utilizando una red de aprendizaje profundo, detecta e interpreta placas matrículas de vehículos. Esta fue obtenida de una investigación publicada en el año 2018. Sobre este sistema se ejecutó un ataque físico de caja negra, manipulando imágenes con pequeñas manchas situadas de manera aleatoria para hacer fallar la interpretación de los dígitos en la matrícula de entrada. Se trabajó con 2 placas matrículas obteniendo en ambas un porcentaje de efectividad en los ataques de 24\% en el primer experimento y 20\% en el segundo. Luego se demostró que el modelo era perfectible en cuanto a su robustez, tanto en la técnica de entrenamiento como en su arquitectura, logrando generar un nuevo modelo clasificador el cual resultó ser robusto en un 100\% a estos ataques formulados en el experimento. Esto demostró la existencia de una relación entre la robustez de un modelo y su arquitectura, como también la importancia de contar con una cantidad amplia de datos de entrenamientos aun cuando las pruebas de efectividad del modelo sean satisfactorias.

